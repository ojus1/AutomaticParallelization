# Towards Automatic Parallelization using Graph Neural Networks

## Abstract
    In recent years, the number of cores in processors have seen a gradual rise, but to make use of them, programmers have to either re-write the single-threaded code or provide "hints" for the compiler to try to parallelize the code block. Automatic Parallelization aims to automate the process of analysing code and parallelizing existing program. This has been achieved upto some extent (Fortran Compilers), but due to the complex code understanding required, there are no efficient solutions. 
    
    We first prove that this problem is NP-Hard, then this paper introduces problem formulation where a Learning agent learns to optimize a greedy algorithm, trained by fitted Q-learning. We exploit the fact that programs can be represented as Directed Acyclic Graphs (DAGs), the problem is formulated as Graph-to-Graph problem, where a learning agent learns to generate several graphs optimized for p processors given a single sequential program as a DAG.
    
    We built a toy dataset of generated DAGs, and we compare models trained on theoretical reward based on Work-Span law, and actual speedup of Python programs generated by DAGs, on 2, 4 and 6 cores, and show that these models can scale to an order of magnitude higher number of nodes than in the training set.

## Paper
Download the draft : [Draft](./draft.pdf)

## Authors
** Surya Kant Sahu
** Durgesh Reddiyaar